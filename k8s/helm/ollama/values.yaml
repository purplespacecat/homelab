# Ollama Helm Chart Custom Values
# Chart: otwld/ollama
# Installation: helm install ollama otwld/ollama -f values.yaml --namespace ollama --create-namespace

# Replica count (1 for single instance)
replicaCount: 1

image:
  repository: ollama/ollama
  pullPolicy: IfNotPresent
  tag: "latest"

# Ollama configuration
ollama:
  # Port Ollama listens on
  port: 11434

  # GPU configuration (set to true if you have NVIDIA/AMD GPUs)
  gpu:
    enabled: false
    type: 'nvidia'  # Options: 'nvidia' or 'amd'
    number: 1

  # Models to pull on startup (optional)
  # Uncomment and add models you want to pre-download
  models:
    []
    # - llama2
    # - mistral
    # - codellama

  # Pull models on container startup
  insecure: false

# Service configuration
service:
  type: LoadBalancer  # Uses MetalLB for local network IP
  port: 11434
  targetPort: 11434
  annotations: {}
  # Add MetalLB specific annotations if needed:
  # metallb.universe.tf/address-pool: default

# Ingress configuration
ingress:
  enabled: true
  className: nginx
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: "0"  # Unlimited for model uploads
    nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
  hosts:
    - host: ollama.192.168.100.202.nip.io  # Adjust to your ingress IP
      paths:
        - path: /
          pathType: Prefix
  tls: []
  # Uncomment for TLS/SSL:
  # tls:
  #  - secretName: ollama-tls
  #    hosts:
  #      - ollama.192.168.100.202.nip.io

# Persistent storage
persistence:
  enabled: true
  storageClassName: nfs-client  # Using NFS dynamic provisioner
  accessModes:
    - ReadWriteOnce
  size: 50Gi  # Adjust based on model sizes
  # Optional: Use existing PVC
  # existingClaim: ollama-data

# Resource limits
resources:
  requests:
    memory: "4Gi"
    cpu: "2000m"
  limits:
    memory: "8Gi"
    cpu: "4000m"

# Liveness and readiness probes
livenessProbe:
  enabled: true
  path: /
  initialDelaySeconds: 60
  periodSeconds: 10
  timeoutSeconds: 5
  successThreshold: 1
  failureThreshold: 6

readinessProbe:
  enabled: true
  path: /
  initialDelaySeconds: 30
  periodSeconds: 5
  timeoutSeconds: 3
  successThreshold: 1
  failureThreshold: 6

# Pod annotations
podAnnotations: {}

# Pod security context
podSecurityContext: {}
  # fsGroup: 2000

# Container security context
securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

# Node selector (for GPU nodes if needed)
nodeSelector: {}
  # nvidia.com/gpu: "true"

# Tolerations
tolerations: []

# Affinity rules
affinity: {}

# Autoscaling (disabled by default for Ollama)
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 3
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

# Extra environment variables
extraEnv: []
  # - name: OLLAMA_DEBUG
  #   value: "1"
  # - name: OLLAMA_ORIGINS
  #   value: "*"

# Service account
serviceAccount:
  create: true
  annotations: {}
  name: ""

# Update strategy
updateStrategy:
  type: Recreate  # Ensures only one instance runs at a time
